{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjK5H60Cr0Tu",
        "outputId": "84708f52-c99a-4a3e-aa22-2bf31a067513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "LjK5H60Cr0Tu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqwE0PG376nA",
        "outputId": "6fc1ab9f-2e4b-481c-b50e-be2139a7dbc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "os.chdir('/content/drive/MyDrive/pk추가')\n",
        "user_df= pd.read_csv(\"./선곡표종합.csv\", encoding = 'utf-8')\n",
        "songDB= pd.read_csv(\"./preprocessed_songDB.csv\", encoding= 'utf-8')\n",
        "tbn= pd.read_csv(\"./tbn_종합.csv\", encoding= 'utf-8')\n",
        "prime = pd.read_excel(\"./tbn_allinone_all_final_url -backup_마지막.xlsx\", usecols= ['pk', 'title'])"
      ],
      "id": "hqwE0PG376nA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QLFXBga7dha"
      },
      "outputs": [],
      "source": [
        "# # 작게 돌리고 싶을 때 (sample 0.01)\n",
        "# user_df_sample = user_df.sample(n=int(len(user_df)*0.01))\n",
        "# tbn_sample = tbn.sample(n=int(len(tbn)*0.01))\n",
        "# user_df_org = user_df.copy()\n",
        "# tbn_org = tbn.copy()\n",
        "# user_df = user_df_sample\n",
        "# tbn = tbn_sample"
      ],
      "id": "5QLFXBga7dha"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRu6e_poOxyz"
      },
      "outputs": [],
      "source": [
        "# FM 모델에서 쓰는 데이터 저장할 path 설정\n",
        "path = '/content/drive/MyDrive/temp/'"
      ],
      "id": "VRu6e_poOxyz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sru-zKewEuJy"
      },
      "source": [
        "##전처리 함수"
      ],
      "id": "Sru-zKewEuJy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhhU_Eu8Vbq7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import datetime as dt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import model_selection\n",
        "from dateutil.parser import parse\n",
        "from collections import OrderedDict\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from collections import Counter \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# FM\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from sklearn import datasets\n",
        "\n",
        "#! pip install xlearn\n",
        "#! pip install gensim\n",
        "#전처리\n",
        "def preprocess_song(df):\n",
        "  df = df.apply(lambda x: re.sub('\\([^)]*\\)',\"\",str(x)))\n",
        "  df = df.apply(lambda x: x.replace(\"'\", \"\"))\n",
        "  df = df.apply(lambda x: x.lower())\n",
        "  df = df.apply(lambda x: re.sub('[^A-Za-z0-9가-힣]', ' ', x))\n",
        "  df = df.apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "  df = df.apply(lambda x: x.strip())\n",
        "  return df\n",
        "\n",
        "#년도 전처리\n",
        "def preprocess_year(result):\n",
        "  result['year'] = list(map(str, result['year']))\n",
        "  result['month'] = list(map(str, result['month']))\n",
        "  result['date'] = list(map(str, result['date']))\n",
        "  return result\n",
        "\n",
        "#방송 시작일 전처리\n",
        "def preprocess_airdate(result):\n",
        "  result['air_date']= result['year'] + \"-\" + result['month'] + '-'+ result['date']\n",
        "  result['air_date'] = list(map(str, result['air_date']))\n",
        "  result['air_date'] = pd.to_datetime(result['air_date'])\n",
        "  # result['air_date'] = result['air_date'].apply(lambda x : parse(x))\n",
        "  return result\n",
        "\n",
        "#장르 전처리\n",
        "def preprocess_genre(result):\n",
        "    result['genre'] = result['genre'].apply(lambda x: sp(x)) \n",
        "    result = result.replace({'genre' : change_value})  \n",
        "    result['genre'] = result['genre'].apply(lambda x : x.strip())\n",
        "    return result\n",
        "\n",
        "\n",
        "# 프로그램별 빈도수 구하기\n",
        "def data_rating(result):\n",
        "    df = pd.DataFrame(data = result.groupby('ProgramName')['song_id'].value_counts())\n",
        "    df.rename(columns = {'song_id': 'rating'}, inplace = True)\n",
        "    dat_ = pd.merge(result, df, on=['ProgramName','song_id'], how = 'inner')\n",
        "    return dat_\n",
        "\n",
        "# MinMaxScaler를 통해 Scaling   \n",
        "def minmax_scaler(dat_):\n",
        "    uniq=dat_['ProgramName'].unique()\n",
        "    n = len(uniq)\n",
        "    for i in range(n):\n",
        "        X = dat_.loc[dat_['ProgramName'] == uniq[i] , ['rating']]\n",
        "        MinMaxScalers = MinMaxScaler()\n",
        "        df_MinMaxScalers=MinMaxScalers.fit_transform(X)\n",
        "        dat_.loc[dat_['ProgramName'] == uniq[i] , ['rating']] = df_MinMaxScalers\n",
        "    return dat_\n",
        "\n",
        "#song_id 별 likes sacling\n",
        "def likes_scaling(dat_):\n",
        "    X = np.reshape([dat_['likes']], (-1, 1))\n",
        "    MinMaxScalers = MinMaxScaler()\n",
        "    MinMaxScalers.fit(X)\n",
        "    df_MinMaxScalers = MinMaxScalers.transform(X)\n",
        "    dat_['likes'] = df_MinMaxScalers\n",
        "    return dat_\n",
        "\n",
        "   \n",
        "#방송일 전처리\n",
        "def parse_(x):\n",
        "    return  parse(x)\n",
        "#장르 전처리\n",
        "def sp(x):\n",
        "    return x.split(',')[0]\n",
        "\n",
        "change_value = {\"포크\" : 'Folk', '랩/힙합' : 'Hip Hop', '댄스' : 'Dance', '록/메탈' : 'Rock', 'R&B/Soul' : 'R&B', '성인가요' : 'adult', '인디음악' : 'Indie'\n",
        "               , '일렉트로니카' : 'Electronica', '재즈' : 'Jazz', '포크/블루스' : 'Folk', '뉴에이지' : 'Newage', '월드뮤직' : 'Worldmusic', '국내영화' : 'Domestic movie'\n",
        "                , '국외영화' : 'Foreign movie', '국내드라마' : 'Domestic drama', '컨트리' : 'Country', '클래식' : 'Classic', '블루스' : 'Blues'\n",
        "                , '국악' : 'Korean classical', '애니메이션/웹툰' : 'Animation', '키즈' : 'Kids', \"국내뮤지컬\" : 'Domestic musicals'\n",
        "                , '국외뮤지컬' : 'Foreign musicals', '게임' : 'Game', '국외드라마' : 'Foreign drama', '불교' : 'Buddhism', '뮤직테라피' : 'Music Therapy', '발라드' : 'Ballade', '-' : 'None' }\n",
        "\n",
        "## target(y) 변수를 만드는 것과 연관이 있어 일단 보류\n",
        "#빈도수 전처리\n",
        "def gb_rating(x):\n",
        "    if x <= 0.001: return 1\n",
        "    if ((x >  0.001) & (x <= 0.05)) : return 2\n",
        "    if ((x >  0.05) & (x <= 0.3)) : return 3\n",
        "    if ((x >  0.3) & (x <= 0.6)) : return 4\n",
        "    return 5\n",
        "\n",
        "#좋아요 전처리                             \n",
        "def gb_likes(x):\n",
        "    if x <= 0.02: return 1\n",
        "    if ((x >  0.02) & (x <= 0.05)) : return 2\n",
        "    if ((x >  0.05) & (x <= 0.1)) : return 3\n",
        "    if ((x >  0.1) & (x <= 0.3)) : return 4\n",
        "    return 5\n",
        "\n",
        "#빈도수와 likes 수를 점수화 시키키\n",
        "def rat_like(dat_):\n",
        "    dat_['rating'] = round(dat_['rating'],4)\n",
        "    dat_['rating'] = dat_['rating'].apply(lambda x: gb_rating(x))\n",
        "    dat_['likes'] = dat_['likes'].apply(lambda x: gb_likes(x))\n",
        "    return dat_\n",
        "\n",
        "################# 22.01.17 추가 함수############## start\n",
        "def pre_w0(x):\n",
        "  for j in x.split('/'): \n",
        "    if str(j) in '맑음구름조금':\n",
        "      return int(1) \n",
        "def pre_w1(x):\n",
        "  for j in x.split('/'): \n",
        "    if str(j) in '흐림구름많음':\n",
        "      return int(1) \n",
        "def pre_w2(x):\n",
        "  for j in x.split('/'): \n",
        "    if str(j) in '안개':\n",
        "      return int(1) \n",
        "def pre_w3(x):\n",
        "  for j in x.split('/'): \n",
        "    if str(j) in '비진눈깨비이슬비소나기':\n",
        "      return int(1) \n",
        "def pre_w4(x):\n",
        "  for j in x.split('/'): \n",
        "    if str(j) in '눈우박소낙눈':\n",
        "      return int(1) \n",
        "\n",
        "def pre_w5(x):\n",
        "  for j in x.split('/'): \n",
        "    if str(j) in '천둥번개':\n",
        "      return int(1) \n",
        "def pre_w6(x):\n",
        "  for j in x.split('/'): \n",
        "    if str(j) in '황사':\n",
        "      return int(1) \n",
        "\n",
        "# 날씨 전처리\n",
        "def preprocess_weather(result):\n",
        "  df2 = pd.DataFrame(index=range(0,len(result)), columns=['weather_clear', 'weather_cloud','weather_fog','weather_rain','weather_snow','weather_thunder','weather_disaster'])\n",
        "  \n",
        "  df2['weather_clear']=result['weather'].apply(lambda x:pre_w0(x))\n",
        "  df2['weather_cloud']=result['weather'].apply(lambda x:pre_w1(x))\n",
        "  df2['weather_fog']=result['weather'].apply(lambda x:pre_w2(x))\n",
        "  df2['weather_rain']=result['weather'].apply(lambda x:pre_w3(x))\n",
        "  df2['weather_snow']=result['weather'].apply(lambda x:pre_w4(x))\n",
        "  df2['weather_thunder']=result['weather'].apply(lambda x:pre_w5(x))\n",
        "  df2['weather_disaster']=result['weather'].apply(lambda x:pre_w6(x))\n",
        "  df2=df2.fillna(0)\n",
        "  df2 = df2.astype('int')\n",
        "  result=pd.concat([result, df2],axis=1)\n",
        "  result=result.fillna(0)\n",
        "  return result\n",
        "\n",
        "\n",
        "################# 22.01.17 추가 함수############## end\n",
        "\n",
        "\n",
        "# #온도를 점수화 시켜줌\n",
        "def temp_bucketize(dat_): # 연평균, 최저기온, 최고기온W\n",
        "  x = dat_['평균기온']\n",
        "  y = dat_['최저기온']\n",
        "  z = dat_['최고기온']\n",
        "\n",
        "  if (y <= -12) : return '0' # 한파\n",
        "  if (z >= 33) : return '8' # 폭염\n",
        "  if (x <= -6) : return '1'\n",
        "  if ((x >  -6) & (x <= 0)) : return '2'\n",
        "  if ((x >  0) & (x <= 6)) : return '3'\n",
        "  if ((x >  6) & (x <= 12)) : return '4'\n",
        "  if ((x >  12) & (x <= 18)) : return '5'\n",
        "  if ((x >  18) & (x <= 24)) : return '6'\n",
        "  if (x > 24) : return '7' \n",
        "\n",
        "# 강수량 : 시간당 강수 기준 (기상청)\n",
        "def rain_bucketize(x):\n",
        "  if x ==0 : return '0'\n",
        "  if ((x >  0) & (x <= 3)) : return '1'\n",
        "  if ((x >  3) & (x <= 15)) : return '2'\n",
        "  if ((x >  15) & (x <= 30)) : return '3'\n",
        "  return '4'\n",
        "\n",
        "def temp_diff_bucketize(x):\n",
        "  if x <= 2.5 : return '0'\n",
        "  if ((x >  2.5) & (x <= 5)) : return '1'\n",
        "  if ((x >  5) & (x <= 7.5)) : return '2'\n",
        "  if ((x >  7.5) & (x <= 10)) : return '3' # 일교차 10 정도가 크다고 판단\n",
        "  if ((x >  10) & (x <= 12.5)) : return '4'\n",
        "  if ((x >  12.5) & (x <= 15)) : return '5'\n",
        "  return '6'\n",
        "  \n",
        "# #온도를 점수화 시켜줌\n",
        "# def temp_bucketize(x):\n",
        "#     if (x <= -5) : return '-10'\n",
        "#     if ((x >  -5) & (x <= 0)) : return '-5'\n",
        "#     if ((x >  0) & (x <= 5)) : return '0'\n",
        "#     if ((x >  5) & (x <= 10)) : return '5'\n",
        "#     if ((x >  10) & (x <= 15)) : return '10'\n",
        "#     if ((x >  15) & (x <= 20)) : return '15'\n",
        "#     return '20'\n",
        "\n",
        "\n",
        "# def rain_bucketize(x):\n",
        "#   if x ==0 : return '0'\n",
        "#   if ((x >  0) & (x <= 1)) : return '1'\n",
        "#   if ((x >  1) & (x <= 3)) : return '2'\n",
        "#   if ((x >  3) & (x <= 5)) : return '3'\n",
        "#   return '4'\n",
        "\n",
        "# def temp_diff_bucketize(x):\n",
        "#   if x <=2.5 : return '0'\n",
        "#   if ((x >  2.5) & (x <= 5)) : return '1'\n",
        "#   if ((x >  5) & (x <= 7.5)) : return '2'\n",
        "#   if ((x >  7.5) & (x <= 10)) : return '3'\n",
        "#   if ((x >  10) & (x <= 12.5)) : return '4'\n",
        "#   if ((x >  12.5) & (x <= 15)) : return '5'\n",
        "#   return '6'\n",
        "                             \n",
        "                                      \n",
        "\n",
        "##분석을 하기 위해서 dict로 저장 -> 피쳐맵을 맵핑하는데 사용\n",
        "def meta2dic(dat_, col):\n",
        "  lst_ = dat_[col].unique()\n",
        "  col_index = {}\n",
        "  for idx, col in enumerate(lst_):\n",
        "      col_index[col] = idx + 1\n",
        "  return col_index\n",
        "\n",
        "\n",
        "\n",
        "##train, test데이터 분리\n",
        "# def train_test_split(dat_):\n",
        "#     train_df = dat_[(dat_['ProgramName'] == 0) & (dat_['air_date'] <= dat_.groupby('ProgramName')['air_date'].quantile(q = 0.7, interpolation = 'nearest')[0])]\n",
        "#     for i in range(1, len(dat_['ProgramName'].unique())):\n",
        "#         df_gb = dat_[(dat_['ProgramName'] == i) & (dat_['air_date'] <= dat_.groupby('ProgramName')['air_date'].quantile(q = 0.7, interpolation = 'nearest')[i])]\n",
        "#         train_df = pd.concat([train_df, df_gb])\n",
        "    \n",
        "#     test_df = dat_[(dat_['ProgramName'] == 0) & (dat_['air_date'] > dat_.groupby('ProgramName')['air_date'].quantile(q = 0.7, interpolation = 'nearest')[0])]\n",
        "#     for i in range(1, len(dat_['ProgramName'].unique())):\n",
        "#         df_gb2 = dat_[(dat_['ProgramName'] == i) & (dat_['air_date'] > dat_.groupby('ProgramName')['air_date'].quantile(q = 0.7, interpolation = 'nearest')[i])]\n",
        "#         test_df = pd.concat([test_df, df_gb2])\n",
        "#     return train_df, test_df\n",
        "\n",
        "def train_test_split(dat_):\n",
        "  data = dat_\n",
        "  target = dat_['ProgramName']\n",
        "\n",
        "  # train_test_split\n",
        "  # train_df, test_df, train_y, test_y = model_selection.train_test_split(data, target, test_size=0.3, shuffle=True, stratify=target, random_state=34)\n",
        "  train_df, test_df, train_y, test_y = model_selection.train_test_split(data, target, test_size=0.3, shuffle=True, random_state=34)\n",
        "\n",
        "  return train_df, test_df\n",
        "\n",
        "        \n",
        "def col_dict(train_df):\n",
        "    col_len_dict = {'ProgramName': len(train_df['ProgramName'].unique()),\n",
        "                'rating': len(train_df['rating'].unique()),\n",
        "                'likes': len(train_df['likes'].unique()), \n",
        "                'genre': len(train_df['genre'].unique()), \n",
        "                # 'weather': len(train_df['weather'].unique()), \n",
        "                'weather_clear': len(train_df['weather_clear'].unique()), \n",
        "                'weather_cloud': len(train_df['weather_cloud'].unique()), \n",
        "                'weather_fog': len(train_df['weather_fog'].unique()), \n",
        "                'weather_rain': len(train_df['weather_rain'].unique()), \n",
        "                'weather_snow': len(train_df['weather_snow'].unique()), \n",
        "                'weather_thunder': len(train_df['weather_thunder'].unique()), \n",
        "                'weather_disaster': len(train_df['weather_disaster'].unique()), \n",
        "                'avg_temp': len(train_df['avg_temp'].unique()),\n",
        "                'release': len(train_df['release'].unique()),\n",
        "                'rain': len(train_df['rain'].unique()),\n",
        "                'temp_diff': len(train_df['temp_diff'].unique()),\n",
        "                'month': len(train_df['month'].unique()),\n",
        "                'song_id': len(train_df['song_id'].unique()),\n",
        "                'day': len(train_df['day'].unique())\n",
        "                }\n",
        "    col_accum_index_dict = {}\n",
        "    cumulative = 0\n",
        "    for key, value in col_len_dict.items():\n",
        "        col_accum_index_dict[key] = cumulative\n",
        "        cumulative = cumulative + value\n",
        "    return col_accum_index_dict\n",
        "\n",
        "\n",
        "# ##모델링 파트\n",
        "# class FactorizationMachine():\n",
        "#     \"\"\"\n",
        "#     This Class is implementation of this paper : https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\n",
        "#     Just a example of FM Algorithm, not for production.\n",
        "#     -----\n",
        "#     Only simple methods are available.\n",
        "#     e.g 1 : batch training, adagrad optimizer, parallel training are not supported.\n",
        "#     e.g 2 : simple optimizer Stochastic Gradient Descent with L2 Regularization.\n",
        "#     e.g 3 : using titanic dataset on local memory.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, k, lr, l2_reg, l2_lambda, epoch, early_stop_window, train_data, valid_data):\n",
        "#         \"\"\"\n",
        "#         :param k: number of latent vector\n",
        "#         :param lr: learning rate\n",
        "#         :param l2_reg: bool parameter for L2 regularization\n",
        "#         :param l2_lambda: lambda of L2 regularization\n",
        "#         :param epoch: training epoch\n",
        "#         :param train_data: path of train data\n",
        "#         :param valid_data: path of valid data\n",
        "#         \"\"\"\n",
        "#         self._k = k\n",
        "#         self._lr = lr\n",
        "#         self._l2_reg = l2_reg\n",
        "#         self._l2_lambda = l2_lambda\n",
        "#         self._epoch = epoch\n",
        "#         self._early_stop_window = early_stop_window\n",
        "#         self._train_file_path = train_data\n",
        "#         self._valid_file_path = valid_data\n",
        "#         self._valid_loss_list = []\n",
        "\n",
        "#     def _load_dataset(self):\n",
        "#         \"\"\"\n",
        "#         1. load dataset to memory from train/valid path\n",
        "#         2. find max index in dataset for w's vector size\n",
        "#         \"\"\"\n",
        "#         # load data\n",
        "#         train_file = open(self._train_file_path, 'r')\n",
        "#         valid_file = open(self._valid_file_path, 'r')\n",
        "#         self._train_data = train_file.read().split('\\n')\n",
        "#         self._valid_data = valid_file.read().split('\\n')\n",
        "#         train_file.close()\n",
        "#         valid_file.close()\n",
        "\n",
        "#         # find max index\n",
        "#         self.feature_max_index = 0\n",
        "#         print(\"Start to init FM vectors.\")\n",
        "#         for row in self._train_data:\n",
        "#             for element in row.split(\" \")[1:]:\n",
        "#                 index = int(element.split(\":\")[0])\n",
        "#                 if self.feature_max_index < index:\n",
        "#                     self.feature_max_index = index\n",
        "\n",
        "#         for row in self._valid_data:\n",
        "#             for element in row.split(\" \")[1:]:\n",
        "#                 index = int(element.split(\":\")[0])\n",
        "#                 if self.feature_max_index < index:\n",
        "#                     self.feature_max_index = index\n",
        "\n",
        "#         # init FM vectors\n",
        "#         self._init_vectors()\n",
        "#         print(\"Finish init FM vectors.\")\n",
        "\n",
        "#     def _init_vectors(self):\n",
        "#         \"\"\"\n",
        "#         1. initialize FM vectors\n",
        "#         2. Conduct naive transformation libsvm format txt data to numpy training sample.\n",
        "#         \"\"\"\n",
        "#         self.w = np.random.randn(self.feature_max_index+1)\n",
        "#         self.v = np.random.randn(self.feature_max_index+1, self._k)\n",
        "#         self.train_x_data = []\n",
        "#         self.train_y_data = np.zeros((len(self._train_data)-1))\n",
        "#         self.valid_x_data = []\n",
        "#         self.valid_y_data = np.zeros((len(self._valid_data)-1))\n",
        "\n",
        "#         # make numpy dataset\n",
        "#         for n, row in enumerate(self._train_data):\n",
        "#             element = row.split(\" \")\n",
        "#             if len(element) > 1:\n",
        "#               self.train_y_data[n] = int(element[0])\n",
        "#               self.train_x_data.append([np.array([int(pair.split(\":\")[0]) for pair in element[1:]]),\n",
        "#                                         np.array([int(pair.split(\":\")[1]) for pair in element[1:]])])\n",
        "\n",
        "#         for n, row in enumerate(self._valid_data):\n",
        "#             element = row.split(\" \")\n",
        "#             if len(element) > 1:\n",
        "#               self.valid_y_data[n] = int(element[0])\n",
        "#               self.valid_x_data.append([np.array([int(pair.split(\":\")[0]) for pair in element[1:]]),\n",
        "#                                         np.array([int(pair.split(\":\")[1]) for pair in element[1:]])])\n",
        "\n",
        "#     def train(self):\n",
        "#         \"\"\"\n",
        "#         Train FM model by Gradient Descent with L2 regularization\n",
        "#         \"\"\"\n",
        "#         self._load_dataset()\n",
        "#         for epoch_num in range(1, self._epoch):\n",
        "#             train_y_hat = self.predict(data=self.train_x_data)\n",
        "#             valid_y_hat = self.predict(data=self.valid_x_data)\n",
        "#             train_loss = self._get_loss(y_data=self.train_y_data, y_hat=train_y_hat)\n",
        "#             valid_loss = self._get_loss(y_data=self.valid_y_data, y_hat=valid_y_hat)\n",
        "#             train_auc = roc_auc_score(self.train_y_data, train_y_hat)\n",
        "#             valid_auc = roc_auc_score(self.valid_y_data, valid_y_hat)\n",
        "#             self._print_learning_info(epoch=epoch_num, train_loss=train_loss, valid_loss=valid_loss,\n",
        "#                                       train_auc=train_auc, valid_auc=valid_auc)\n",
        "#             if self._check_early_stop(valid_loss=valid_loss):\n",
        "#                 print(\"Early stop at epoch:\", epoch_num)\n",
        "#                 return 0\n",
        "\n",
        "#             self._stochastic_gradient_descent(self.train_x_data, self.train_y_data)\n",
        "\n",
        "#     def predict(self, data):\n",
        "#         \"\"\"\n",
        "#         Implementation of FM model's equation on O(kmd)\n",
        "#         -----\n",
        "#         Numpy array shape : (n, [index of md], [value of md])\n",
        "#         md : none-zero feature\n",
        "#         \"\"\"\n",
        "#         num_data = len(data)\n",
        "#         scores = np.zeros(num_data)\n",
        "#         for n in range(num_data):\n",
        "#             feat_idx = data[n][0]\n",
        "#             val = data[n][1]\n",
        "\n",
        "#             # linear feature score\n",
        "#             linear_feature_score = np.sum(self.w[feat_idx] * val)\n",
        "\n",
        "#             # factorized feature score\n",
        "#             vx = self.v[feat_idx] * (val.reshape(-1, 1))\n",
        "#             cross_sum = np.sum(vx, axis=0)\n",
        "#             square_sum = np.sum(vx * vx, axis=0)\n",
        "#             cross_feature_score = 0.5 * np.sum(np.square(cross_sum) - square_sum)\n",
        "\n",
        "#             # Model's equation\n",
        "#             scores[n] = linear_feature_score + cross_feature_score\n",
        "\n",
        "#         # Sigmoid transformation for binary classification\n",
        "#         scores = 1.0 / (1.0 + np.exp(-scores))\n",
        "#         return scores\n",
        "\n",
        "#     def _get_loss(self, y_data, y_hat):\n",
        "#         \"\"\"\n",
        "#         Calculate loss with L2 regularization (two type of coeficient - w,v)\n",
        "#         \"\"\"\n",
        "#         l2_norm = 0\n",
        "#         if self._l2_reg:\n",
        "#             w_norm = np.sqrt(np.sum(np.square(self.w)))\n",
        "#             v_norm = np.sqrt(np.sum(np.square(self.v)))\n",
        "#             l2_norm = self._l2_lambda * (w_norm + v_norm)\n",
        "#         return -1 * np.sum( (y_data * np.log(y_hat)) + ((1 - y_data) * np.log(1 - y_hat)) ) + l2_norm\n",
        "\n",
        "#     def _check_early_stop(self, valid_loss):\n",
        "#         self._valid_loss_list.append(valid_loss)\n",
        "#         if len(self._valid_loss_list) > 5:\n",
        "#             prev_loss = self._valid_loss_list[len(self._valid_loss_list) - self._early_stop_window]\n",
        "#             curr_loss = valid_loss\n",
        "#             if prev_loss < curr_loss:\n",
        "#                 return True\n",
        "#         return False\n",
        "\n",
        "#     def _print_learning_info(self, epoch, train_loss, valid_loss, train_auc, valid_auc):\n",
        "#         print(\"epoch:\", epoch, \"||\", \"train_loss:\", train_loss, \"||\", \"valid_loss:\", valid_loss,\n",
        "#               \"||\", \"Train AUC:\", train_auc, \"||\", \"Test AUC:\", valid_auc)\n",
        "\n",
        "\n",
        "#     def _stochastic_gradient_descent(self, x_data, y_data):\n",
        "#         \"\"\"\n",
        "#         Update each coefs (w, v) by Gradient Descent\n",
        "#         \"\"\"\n",
        "#         for data, y in zip(x_data, y_data):\n",
        "#             feat_idx = data[0]\n",
        "#             val = data[1]\n",
        "#             vx = self.v[feat_idx] * (val.reshape(-1, 1))\n",
        "\n",
        "#             # linear feature score\n",
        "#             linear_feature_score = np.sum(self.w[feat_idx] * val)\n",
        "\n",
        "#             # factorized feature score\n",
        "#             vx = self.v[feat_idx] * (val.reshape(-1, 1))\n",
        "#             cross_sum = np.sum(vx, axis=0)\n",
        "#             square_sum = np.sum(vx * vx, axis=0)\n",
        "#             cross_feature_score = 0.5 * np.sum(np.square(cross_sum) - square_sum)\n",
        "\n",
        "#             # Model's equation\n",
        "#             score = linear_feature_score + cross_feature_score\n",
        "#             y_hat = 1.0 / (1.0 + np.exp(-score))\n",
        "#             cost = y_hat - y\n",
        "\n",
        "#             if self._l2_reg:\n",
        "#                 self.w[feat_idx] = self.w[feat_idx] - cost * self._lr * (val + self._l2_lambda * self.w[feat_idx])\n",
        "#                 self.v[feat_idx] = self.v[feat_idx] - cost * self._lr * ((sum(vx) * (val.reshape(-1, 1)) - (vx * (val.reshape(-1, 1)))) + self._l2_lambda * self.v[feat_idx])\n",
        "#             else:\n",
        "#                 self.w[feat_idx] = self.w[feat_idx] - cost * self._lr * val\n",
        "#                 self.v[feat_idx] = self.v[feat_idx] - cost * self._lr * (sum(vx) * (val.reshape(-1, 1)) - (vx * (val.reshape(-1, 1))))\n",
        "                             \n",
        "\n",
        "#프로그램별 노래를 rating의 오름차순으로 정렬\n",
        "def get_music_list_sort_by_rating(x):\n",
        "    return x.sort_values(['rating'])['song_id'].tolist()\n",
        "\n",
        "#최근 틀었던 노래\n",
        "def get_recent_song_list_sort_by_time(x, k):\n",
        "        return x.sort_values(['air_date'])['song_id'].tolist()[-k:]\n",
        "\n",
        "\n",
        "##인기많은 곡을 중복 제거해서 뽑기\n",
        "def get_meta_popular_list_candidate(x, k):\n",
        "    song_id_list = x.sort_values(by=['mean'], ascending=False)['song_id'].tolist()\n",
        "    song_id_list = list(OrderedDict.fromkeys(song_id_list ))\n",
        "\n",
        "    return song_id_list[:k]\n",
        "\n",
        "def most_frequent(x): \n",
        "    occurence_count = Counter(x) \n",
        "    return occurence_count.most_common(1)[0][0] \n",
        "\n",
        "################# 22.01.17 추가 함수############## start\n",
        "\n",
        "def get_meta_popular_list_candidate_weather(x, k):\n",
        "  total_list=[]\n",
        "  for weather in ['weather_clear','weather_cloud','weather_fog','weather_rain','weather_snow','weather_thunder','weather_disaster']:\n",
        "    song_id_list = x[x[weather]==1].sort_values(by=['mean'], ascending=False)['song_id'].tolist()\n",
        "    song_id_list = list(OrderedDict.fromkeys(song_id_list ))\n",
        "    total_list.append(song_id_list[:k])\n",
        "\n",
        "  return total_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Feature Mapping - Profiling 피처맵 맵핑\n",
        "def featre_mapping(dat_, df):\n",
        "    # weather_index = meta2dic(dat_, 'weather')\n",
        "    # avg_temp_index = meta2dic(dat_, 'avg_temp')\n",
        "    # rain_index =  meta2dic(dat_, 'rain')\n",
        "    # temp_diff_index =  meta2dic(dat_, 'temp_diff')\n",
        "    # month_index =  meta2dic(dat_, 'month')\n",
        "    day_index =  meta2dic(dat_, 'day')\n",
        "    genre_index = meta2dic(dat_, 'genre')\n",
        "    year_index = meta2dic(dat_, 'release')\n",
        "\n",
        "    df = df[['ProgramName', 'rating', 'likes','weather_clear','weather_cloud','weather_fog','weather_rain','weather_snow','weather_thunder','weather_disaster', 'avg_temp', 'genre', 'release', 'song_id', 'rain', 'temp_diff', 'month', 'day']]\n",
        "    df['genre'] = df['genre'].apply(lambda x: genre_index[x])\n",
        "    df['release'] = df['release'].apply(lambda x: year_index[x])\n",
        "    # df['weather'] = df['weather'].apply(lambda x: weather_index[x])\n",
        "    # df['avg_temp'] = df['avg_temp'].apply(lambda x: avg_temp_index[x])\n",
        "    # df['rain'] = df['rain'].apply(lambda x: rain_index[x])\n",
        "    # df['temp_diff'] = df['temp_diff'].apply(lambda x: temp_diff_index[x])\n",
        "    # df['month'] = df['month'].apply(lambda x: month_index[x])\n",
        "    df['day'] = df['day'].apply(lambda x: day_index[x])\n",
        "    df['y'] = df['rating'].apply(lambda x : 1 if x >= 2 else 0)\n",
        "    return df\n",
        "\n",
        "    ##추천결과 평가\n",
        "def eval_mapping(dat_):\n",
        "    # weather_index = meta2dic(dat_, 'weather')\n",
        "    # avg_temp_index = meta2dic(dat_, 'avg_temp')\n",
        "    # rain_index =  meta2dic(dat_, 'rain')\n",
        "    # temp_diff_index =  meta2dic(dat_, 'temp_diff')\n",
        "    # month_index =  meta2dic(dat_, 'month')\n",
        "    day_index =  meta2dic(dat_, 'day')\n",
        "    genre_index = meta2dic(dat_, 'genre')\n",
        "    year_index = meta2dic(dat_, 'release')\n",
        "\n",
        "    program_df = dat_[['ProgramName', 'weather_clear','weather_cloud','weather_fog','weather_rain','weather_snow','weather_thunder','weather_disaster','avg_temp', 'rain', 'temp_diff', 'month', 'day']].copy()\n",
        "    # program_df['weather'] = dat_['weather'].apply(lambda x: weather_index[x])\n",
        "    # program_df['avg_temp'] = dat_['avg_temp'].apply(lambda x: avg_temp_index[x])\n",
        "    # program_df['rain'] = dat_['rain'].apply(lambda x: rain_index[x])\n",
        "    # program_df['temp_diff'] = dat_['temp_diff'].apply(lambda x: temp_diff_index[x])\n",
        "    # program_df['month'] = dat_['month'].apply(lambda x: month_index[x])\n",
        "    program_df['day'] = dat_['day'].apply(lambda x: day_index[x])\n",
        "\n",
        "    song_df = dat_[['song_id','release', 'genre']].copy()\n",
        "    song_df['release'] = song_df['release'].apply(lambda x: year_index[x])\n",
        "    song_df['genre'] = song_df['genre'].apply(lambda x: genre_index[x])\n",
        "\n",
        "    return program_df, song_df\n",
        "\n",
        "################# 22.01.17~18 추가 함수############## end\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#최근 틀었던 노래와 유사한곡을 추출하기 위한 함수.\n",
        "def get_similar_items(item2item, x, k):\n",
        "    similar_items = []\n",
        "    for song_id in x:\n",
        "        if song_id in item2item:\n",
        "            similar_items.append(item2item[song_id][:k])\n",
        "    return [item for items in similar_items for item in items]\n",
        "\n",
        "###예측값 추출\n",
        "def make_libsvm_row(col_accum_index_dict, program_index_dict, song_index_dict, uid, sid):\n",
        "  user_id = str(col_accum_index_dict['ProgramName'] + uid)\n",
        "  weather_clear = str(col_accum_index_dict['weather_clear'] + program_index_dict['weather_clear'][uid])\n",
        "  weather_cloud = str(col_accum_index_dict['weather_cloud'] + program_index_dict['weather_cloud'][uid])\n",
        "  weather_fog = str(col_accum_index_dict['weather_fog'] + program_index_dict['weather_fog'][uid])\n",
        "  weather_rain = str(col_accum_index_dict['weather_rain'] + program_index_dict['weather_rain'][uid])\n",
        "  weather_snow = str(col_accum_index_dict['weather_snow'] + program_index_dict['weather_snow'][uid])\n",
        "  weather_thunder = str(col_accum_index_dict['weather_thunder'] + program_index_dict['weather_thunder'][uid])\n",
        "  weather_disaster = str(col_accum_index_dict['weather_disaster'] + program_index_dict['weather_disaster'][uid])\n",
        "  avg_temp = str(col_accum_index_dict['avg_temp'] + int(program_index_dict['avg_temp'][uid]))\n",
        "  rain = str(col_accum_index_dict['rain'] + int(program_index_dict['rain'][uid]))\n",
        "  temp_diff = str(col_accum_index_dict['temp_diff'] + int(program_index_dict['temp_diff'][uid]))\n",
        "  month = str(col_accum_index_dict['month'] + int(program_index_dict['month'][uid]) - 1)\n",
        "  day = str(col_accum_index_dict['day'] + program_index_dict['day'][uid] - 1)\n",
        "  # song_id = str(col_accum_index_dict['song_id'] + int(sid) - 1)    #???????????????\n",
        "  release = str(col_accum_index_dict['release'] + song_index_dict['release'][int(sid)] - 1)\n",
        "  genre = str(col_accum_index_dict['genre'] + song_index_dict['genre'][int(sid)] - 1)   \n",
        "  return \" \".join([user_id, weather_clear,weather_cloud,weather_fog,weather_rain,weather_snow,weather_thunder,weather_disaster, release, genre, avg_temp, rain, temp_diff, month, day ])\n",
        "\n",
        "############################ 22.01.17 일부 수정 ###########################################\n",
        "def recomend_list(recommendations, dat_, program_id):\n",
        "    recomend = list(map(int, list(recommendations[program_id])))\n",
        "    reco = dat_[dat_['song_id'].isin(recomend)][['songName', 'artistName', 'genre', 'release', 'weather','likes', 'region', 'song_id', 'month', 'day', 'release_before', 'air_date','weather_clear','weather_cloud','weather_fog','weather_rain','weather_snow','weather_thunder','weather_disaster']].drop_duplicates()\n",
        "    return reco \n",
        "\n",
        "############################ 22.01.24 수정(노래확인함수) ###########################################\n",
        "\n",
        "def get_song_info(df, song_id_list):\n",
        "  program_check=[]\n",
        "  for id in song_id_list:\n",
        "    program_check.append((df[(df['song_id'] == id)][['songName', 'artistName' ,'genre', 'release']].drop_duplicates()))\n",
        "  df_program_check=pd.concat(program_check,ignore_index=True)\n",
        "\n",
        "  return df_program_check"
      ],
      "id": "UhhU_Eu8Vbq7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델링 함수"
      ],
      "metadata": {
        "id": "LrJfsIUw9IKP"
      },
      "id": "LrJfsIUw9IKP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR2GZWRvVhZG"
      },
      "source": [
        "### 전처리"
      ],
      "id": "PR2GZWRvVhZG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb1395e8"
      },
      "outputs": [],
      "source": [
        "def preprocess(df, songDB, tbn):\n",
        "    tbn_program = tbn.copy()\n",
        "    #제목과 가수 전처리\n",
        "    df['songName'] = preprocess_song(df['songName'])   \n",
        "    df['artistName'] = preprocess_song(df['artistName'])    # 30 second\n",
        "\n",
        "    df = df.dropna(how='any',axis=0)\n",
        "\n",
        "    #songdb와 merge\n",
        "    result = pd.merge(df, songDB, on = ['songName', 'artistName']  )\n",
        "    #년도 전처리\n",
        "    result = preprocess_year(result)\n",
        "    #방송 시작일 전처리\n",
        "    result = preprocess_airdate(result)       # 100 second\n",
        "    #장르 전처리\n",
        "    result = preprocess_genre(result)\n",
        "    # 프로그램별 빈도수 구하기\n",
        "    dat_ = data_rating(result)\n",
        "    # MinMaxScaler를 통해 Scaling   \n",
        "    dat_ = minmax_scaler(dat_)                # 200 second\n",
        "    #song_id 별 likes sacling\n",
        "    dat_ = likes_scaling(dat_)\n",
        "\n",
        "    #프로그램 label encoding\n",
        "    le = LabelEncoder()\n",
        "    dat_['ProgramName']=le.fit_transform(dat_['ProgramName'])\n",
        "    programs_id = []\n",
        "    le.classes_\n",
        "\n",
        "    #tbn의 Program_id 저장\n",
        "    inverse_id = list(le.inverse_transform(list(dat_['ProgramName'].unique())))\n",
        "    for i in range(len(inverse_id)):\n",
        "        if inverse_id[i] in list(tbn_program['ProgramName'].unique()):\n",
        "            programs_id.append(list(dat_['ProgramName'].unique())[i])\n",
        "\n",
        "    #빈도수와 likes 수를 점수화 시키키\n",
        "    dat_ =  rat_like(dat_)  \n",
        "    dat_ = preprocess_weather(dat_)\n",
        "    ### 탐색적으로 다양한 추천 후보군 생성\n",
        "    ##다양한 Meta 후보군 생성\n",
        "    # dat_['avg_temp'] = dat_['평균기온'].apply(lambda x: temp_bucketize(x))\n",
        "    dat_['avg_temp'] = dat_.apply(temp_bucketize, axis = 1)\n",
        "\n",
        "    dat_.loc[dat_['강수량(mm)'] == '-', '강수량(mm)'] = 0\n",
        "    dat_['강수량(mm)'] = dat_['강수량(mm)'].apply(lambda x : float(x))\n",
        "    dat_['rain'] = dat_['강수량(mm)'].apply(lambda x: rain_bucketize(x))\n",
        "\n",
        "    dat_['temp_diff'] = dat_['최고기온'] -  dat_['최저기온']                             \n",
        "    dat_['temp_diff'] = dat_['temp_diff'].apply(lambda x: temp_diff_bucketize(x))\n",
        "\n",
        "\n",
        "    ##train, test데이터 분리\n",
        "    train_df, test_df = train_test_split(dat_)\n",
        "    train_final = train_df.copy()\n",
        "    test_final = test_df.copy()\n",
        "\n",
        "    train_df = train_df.reset_index(drop = True)\n",
        "    test_df = test_df.reset_index(drop = True)\n",
        "\n",
        "    ## Feature Mapping - Profiling 피처맵 맵핑\n",
        "    train_df = featre_mapping(dat_, train_df)\n",
        "    test_df = featre_mapping(dat_, test_df)\n",
        "\n",
        "    col_accum_index_dict = col_dict(train_df)\n",
        "    return dat_, programs_id, le, train_df, test_df, col_accum_index_dict, train_final, test_final"
      ],
      "id": "cb1395e8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### func_fm"
      ],
      "metadata": {
        "id": "M6SQaNeLwDTN"
      },
      "id": "M6SQaNeLwDTN"
    },
    {
      "cell_type": "code",
      "source": [
        "# class FM(tf.keras.Model):\n",
        "#     def __init__(self):\n",
        "#         super(FM, self).__init__()\n",
        "\n",
        "#         # 모델의 파라미터 정의\n",
        "#         self.w_0 = tf.Variable([0.0])\n",
        "#         self.w = tf.Variable(tf.zeros([p]))\n",
        "#         self.V = tf.Variable(tf.random.normal(shape=(p, k)))\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         linear_terms = tf.reduce_sum(tf.math.multiply(self.w, inputs), axis=1)\n",
        "\n",
        "#         interactions = 0.5 * tf.reduce_sum(\n",
        "#             tf.math.pow(tf.matmul(inputs, self.V), 2)\n",
        "#             - tf.matmul(tf.math.pow(inputs, 2), tf.math.pow(self.V, 2)),\n",
        "#             1,\n",
        "#             keepdims=False\n",
        "#         )\n",
        "\n",
        "#         y_hat = self.w_0 + linear_terms + interactions\n",
        "#         # return y_hat\n",
        "#         return y_hat, self.w, self.V\n",
        "\n",
        "        \n",
        "def train_on_batch(model, optimizer, accuracy, inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred,w,V = model(inputs)\n",
        "        # loss = tf.keras.losses.binary_crossentropy(from_logits=False,\n",
        "        #                                            y_true=targets,\n",
        "        #                                            y_pred=y_pred)\n",
        "        loss = tf.keras.losses.MSE(y_true=targets,y_pred=y_pred)\n",
        "\n",
        "        # print(w)\n",
        "        # print(V)\n",
        "####################\n",
        "        error = tf.keras.losses.MSE(y_true=targets,y_pred=y_pred)\n",
        "        regularizer_w = tf.nn.l2_loss(w)\n",
        "        regularizer_v = tf.nn.l2_loss(V)\n",
        "        loss = (error + 0.001 * regularizer_w + 0.001 * regularizer_v)\n",
        "\n",
        "#####################\n",
        "\n",
        "        # lambda_w = tf.constant(0.001, name='lambda_w')\n",
        "        # lambda_v = tf.constant(0.001, name='lambda_v')\n",
        "        \n",
        "\n",
        "        # l2_norm = (tf.multiply(lambda_w, tf.pow(w, 2))+\n",
        "        #                 tf.multiply(lambda_v, tf.pow(V, 2)))\n",
        "        # print(l2_norm)\n",
        "\n",
        "        # l2_norm = (tf.math.scalar_mul(0.001, tf.pow(w, 2))+\n",
        "        #                 tf.math.scalar_mul(0.001, tf.pow(V, 2)))\n",
        "\n",
        "        # l2_norm = (tf.reduce_sum(\n",
        "        #             tf.add(\n",
        "        #                 tf.multiply(lambda_w, tf.pow(w, 2)),\n",
        "        #                 tf.multiply(lambda_v, tf.pow(V, 2)))))\n",
        "        # loss = tf.add(error, l2_norm)\n",
        "\n",
        "    \n",
        "    # loss를 모델의 파라미터로 편미분하여 gradients를 구한다.\n",
        "    grads = tape.gradient(target=loss, sources=model.trainable_variables)\n",
        "\n",
        "    # apply_gradients()를 통해 processed gradients를 적용한다.\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # accuracy: update할 때마다 정확도는 누적되어 계산된다.\n",
        "    accuracy.update_state(targets, y_pred)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "# 반복 학습 함수\n",
        "def train(X_train, X_test, Y_train, Y_test, p, k, epochs, batch=16, learning_rate=0.001, threshold=0.5):\n",
        "#    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y)\n",
        "    # X_train, X_test, Y_train, Y_test = xtrain, xtest, ytrain, ytest\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "        (tf.cast(X_train, tf.float32), tf.cast(Y_train, tf.float32))).shuffle(5000).batch(batch)\n",
        "\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "        (tf.cast(X_test, tf.float32), tf.cast(Y_test, tf.float32))).shuffle(2000).batch(batch)\n",
        "\n",
        "    print('data transformaiton completed')\n",
        "\n",
        "    model = FM(p, k)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    accuracy = RootMeanSquaredError()\n",
        "    loss_history = []\n",
        "\n",
        "    #checkpoint 생성\n",
        "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer = optimizer, model = model)\n",
        "    manager = tf.train.CheckpointManager(ckpt, '/content/drive/MyDrive/temp/fm_check', max_to_keep=3)\n",
        "    \n",
        "    ckpt.restore(manager.latest_checkpoint)\n",
        "    if manager.latest_checkpoint:\n",
        "      print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "    else:\n",
        "      print(\"Initializing from scratch.\")\n",
        "      \n",
        "    for i in range(epochs):\n",
        "      for x, y in train_ds:\n",
        "          loss = train_on_batch(model, optimizer, accuracy, x, y)\n",
        "          loss_history.append(loss)\n",
        "      ckpt.step.assign_add(1)\n",
        "      if int(ckpt.step) % 1 == 0:\n",
        "        save_path = manager.save()\n",
        "        print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
        "      # status.assert_consumed()\n",
        "      # checkpoint.save(checkpoint_prefix)\n",
        "\n",
        "      # print(accuracy)\n",
        "\n",
        "      if i % 1== 0:\n",
        "          # print(\"스텝 {:03d}에서 누적 평균 손실: {:.4f} 정확도: {:.4f}\".format(i, np.mean(loss_history), accuracy.result().numpy()))\n",
        "          print(\"스텝 {:03d}에서 RMSE: {:.4f}\".format(i, accuracy.result().numpy()))\n",
        "\n",
        "          # print(\"스텝 {:03d}에서 누적 정확도: {:.4f}\".format(i, accuracy.result().numpy()))\n",
        "\n",
        "\n",
        "    test_accuracy = RootMeanSquaredError()\n",
        "\n",
        "    y_pred_list = []\n",
        "    for x, y in test_ds:\n",
        "        y_pred,w,V = model(x)\n",
        "#        print(y_pred)\n",
        "        test_accuracy.update_state(y, y_pred)\n",
        "        y_pred_list.append(y_pred)\n",
        "\n",
        "    print(\"테스트 RMSE: {:.4f}\".format(test_accuracy.result().numpy()))\n",
        "    return y_pred_list, model"
      ],
      "metadata": {
        "id": "E_5oqHzTV07e"
      },
      "id": "E_5oqHzTV07e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FM(tf.keras.Model):\n",
        "    def __init__(self, p, k):\n",
        "        super(FM, self).__init__()\n",
        "\n",
        "        # 모델의 파라미터 정의\n",
        "        self.w_0 = tf.Variable([0.0])\n",
        "        self.w = tf.Variable(tf.zeros([p]))\n",
        "        self.V = tf.Variable(tf.random.normal(shape=(p, k)))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        linear_terms = tf.reduce_sum(tf.math.multiply(self.w, inputs), axis=1)\n",
        "\n",
        "        interactions = 0.5 * tf.reduce_sum(\n",
        "            tf.math.pow(tf.matmul(inputs, self.V), 2)\n",
        "            - tf.matmul(tf.math.pow(inputs, 2), tf.math.pow(self.V, 2)),\n",
        "            1,\n",
        "            keepdims=False\n",
        "        )\n",
        "\n",
        "        y_hat = self.w_0 + linear_terms + interactions\n",
        "        # return y_hat\n",
        "        return y_hat, self.w, self.V"
      ],
      "metadata": {
        "id": "4sJOkg3ohVR7"
      },
      "id": "4sJOkg3ohVR7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 모델링\n",
        "\n",
        "def func_fm(train_df, test_df):\n",
        "  # GPU 확인\n",
        "  tf.config.list_physical_devices('GPU')\n",
        "\n",
        "  # 자료형 선언\n",
        "  tf.keras.backend.set_floatx('float32')\n",
        "\n",
        "  # 데이터 로드\n",
        "  scaler = MinMaxScaler()\n",
        "  xtrain = train_df.drop(['rating', 'y','likes'], axis = 1)\n",
        "  ytrain = train_df.loc[:,'rating']\n",
        "  xtest = test_df.drop(['rating', 'y', 'likes'], axis = 1)\n",
        "  ytest = test_df.loc[:,'rating']\n",
        "\n",
        "  xtrain = scaler.fit_transform(xtrain)\n",
        "  xtest = scaler.transform(xtest)\n",
        "\n",
        "  n = xtrain.shape[0] + xtest.shape[0]\n",
        "  p = xtrain.shape[1]  # 예측 변수의 개수\n",
        "  k = 12  # 잠재 변수의 개수\n",
        "  batch_size = 16\n",
        "  epochs = 10\n",
        "\n",
        "  # tf.debugging.set_log_device_placement(True)\n",
        "  # 텐서를 GPU에 할당\n",
        "  with tf.device('/GPU:0'):\n",
        "    fm_model = train(xtrain, xtest, ytrain, ytest, p, k, 5, batch=512)\n",
        "\n",
        "  # fm_model[1].save_weights('/content/drive/MyDrive/temp/fm_check')\n",
        "  return fm_model, scaler"
      ],
      "metadata": {
        "id": "oqv7ws5lV34V"
      },
      "id": "oqv7ws5lV34V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fm_model.load_weights('/content/drive/MyDrive/temp/fm_check')"
      ],
      "metadata": {
        "id": "jDDhgEU7hcu4"
      },
      "id": "jDDhgEU7hcu4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model[1].save('/content/drive/MyDrive/temp/mymodel')\n",
        "# tf.keras.models.load_model('/content/drive/MyDrive/temp/mymodel')"
      ],
      "metadata": {
        "id": "tXhKqkGTbdeh"
      },
      "id": "tXhKqkGTbdeh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hrus4qrbydz-"
      },
      "source": [
        "###1. func1 : 연관노래 k개 "
      ],
      "id": "Hrus4qrbydz-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhXtYQSyCBgo"
      },
      "outputs": [],
      "source": [
        "def func1_song_similar(train_final, dat_):\n",
        "  # #train_final, test_final이용\n",
        "  train_rate_list = train_final.groupby('ProgramName')[['song_id', 'rating']].apply(lambda x: get_music_list_sort_by_rating(x))\n",
        "\n",
        "  ##word2vec학습\n",
        "  ##song_id별 발매일과 장르를 dict로 만들어줌\n",
        "  song_meta_dict = train_final.set_index('song_id')[['release', 'genre']].to_dict()\n",
        "  song2vec_dataset = []\n",
        "  ##song_id, 발매일, 장르를 word2vec을 학습하기 위한 모형으로 만들어줌\n",
        "  for song_list in train_rate_list:\n",
        "      meta_list = []\n",
        "      for music_id in song_list:\n",
        "          word_meta_1 = \"song_id:\" + str(music_id)\n",
        "          word_meta_2 = \"year:\" + str(song_meta_dict['release'][music_id])\n",
        "          word_meta_3 = \"genre:\" + str(song_meta_dict['genre'][music_id])\n",
        "          meta_list.append(word_meta_1)\n",
        "          meta_list.append(word_meta_2)\n",
        "          meta_list.append(word_meta_3)\n",
        "\n",
        "      song2vec_dataset.append(meta_list)\n",
        "      \n",
        "  ##word2vec학습 -> 곡별 유사도를 구할 수 있음.\n",
        "  model = Word2Vec(song2vec_dataset,\n",
        "                    window=6,  # 주변 word의 윈도우\n",
        "                    sg=1,  # skip-gram OR cbow\n",
        "                    hs=0,  # hierarchical softmax OR negative sampling\n",
        "                    negative=20,  # negative sampling 파라미터\n",
        "                    min_count=1  # word의 등장 최소 횟수\n",
        "                  )\n",
        "\n",
        "  # #Embedding - 일부 데이터로 song2Vec 학습 결과 확인\n",
        "  #최근 틀었던 노래 추출\n",
        "  recent_program_song_list = train_final.groupby('ProgramName')[['song_id', 'air_date']].apply(lambda x: get_recent_song_list_sort_by_time(x, 10))\n",
        "  # 노래별 연관 노래 k개씩 추출\n",
        "  k = 20\n",
        "  #not_in_count = 0\n",
        "  item2item = {}\n",
        "  song_unique=dat_['song_id'].unique()\n",
        "  for song_id in song_unique:                 \n",
        "      item2item[song_id] = []\n",
        "      try:\n",
        "          #곡과 유사한 곡들의 리스트를 추출\n",
        "          sim_list = model.wv.most_similar(\"song_id:\" + str(song_id), topn=k+20)\n",
        "          for song_tup in sim_list:\n",
        "              tup_info = song_tup[0].split(\":\")\n",
        "              if (tup_info[0] == \"song_id\") and (len(item2item[song_id]) < 20):\n",
        "                  item2item[song_id].append(tup_info[1])\n",
        "              if (len(item2item[song_id]) >= 20):\n",
        "                break\n",
        "      except:\n",
        "          pass\n",
        "          #not_in_count += 1\n",
        "          #print(\"word\", str(song_id) ,\"not in vocabulary\")\n",
        "\n",
        "  recent_song_similar_items = recent_program_song_list.apply(lambda x: get_similar_items(item2item, x, k=30))\n",
        "\n",
        "  return recent_song_similar_items"
      ],
      "id": "JhXtYQSyCBgo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktt3jzC5zBYg"
      },
      "source": [
        "###2. func2 : 인기있는 곡 "
      ],
      "id": "Ktt3jzC5zBYg"
    },
    {
      "cell_type": "code",
      "source": [
        "def func2_song_rating(dat_, train_final):\n",
        "  mean_rating = train_final.groupby('song_id')['rating'].agg(['mean', 'count'])\n",
        "\n",
        "  ###빈도수의 평균이 높은 상위 30개 노래\n",
        "  popular_song_list = mean_rating[mean_rating['count']>5]['mean'].sort_values(ascending=False).index[:30].tolist()\n",
        "\n",
        "  ##장르, 연도, 날씨를 meta로 하여, meta별 빈도수 평균이 높은 상위 30개의 노래\n",
        "  merge_df = pd.merge(mean_rating, dat_, on='song_id')\n",
        "  genre_popular = merge_df.groupby('genre').apply(lambda x: get_meta_popular_list_candidate(x, k=50))\n",
        "  year_popular = merge_df.groupby('release').apply(lambda x: get_meta_popular_list_candidate(x, k=50))\n",
        "  weather_popular = get_meta_popular_list_candidate_weather(merge_df, k=50)\n",
        "\n",
        "  train_year_list = train_final.groupby(by=['ProgramName','release'], as_index=False)['song_id'].count()\n",
        "  train_year_list=train_year_list.loc[train_year_list.groupby(['ProgramName'])['song_id'].idxmax()]\n",
        "\n",
        "\n",
        "  train_genre_list=train_final.groupby(by=['ProgramName','genre'], as_index=False)['song_id'].count()\n",
        "  train_genre_list=train_genre_list.loc[train_genre_list.groupby(['ProgramName'])['song_id'].idxmax()]\n",
        "\n",
        "  tf_weather=train_final.loc[:,['ProgramName','weather_clear','weather_cloud','weather_fog','weather_rain','weather_snow','weather_thunder','weather_disaster']]\n",
        "  train_weather_list=tf_weather.groupby(by=['ProgramName'], as_index=False).agg(['sum'])\n",
        "\n",
        "  weather_list=[]\n",
        "\n",
        "  for idx,count_row in train_weather_list.iterrows():\n",
        "    max=-1\n",
        "    idx_check=-1\n",
        "    for count_num,temp_idx in zip(count_row,range(len(count_row))) :\n",
        "      if max<count_num:\n",
        "        max=count_num\n",
        "        idx_check=temp_idx\n",
        "    weather_list.append(idx_check)\n",
        "\n",
        "  recommendations = dict()\n",
        "\n",
        "  for idx,[a,year],[b,genre],wt in zip(range(len(train_year_list)),train_year_list.iterrows(),train_genre_list.iterrows(),weather_list):\n",
        "    recommendations[idx]= list(set(year_popular[year[1]] + genre_popular[genre[1]]+weather_popular[wt]))\n",
        "\n",
        "  return recommendations"
      ],
      "metadata": {
        "id": "CKkBIbYTOt1w"
      },
      "id": "CKkBIbYTOt1w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS4f4yzOE5sk"
      },
      "outputs": [],
      "source": [
        "def combine_func1_func2(train_final, dat_):\n",
        "  #train_final, test_final이용\n",
        "  # train_rate_list = train_final.groupby('ProgramName')[['song_id', 'rating']].apply(lambda x: get_music_list_sort_by_rating(x))\n",
        "  recent_song_similar_items = func1_song_similar(train_final, dat_)\n",
        "  recommendations = func2_song_rating(dat_, train_final)\n",
        "\n",
        "  return recent_song_similar_items, recommendations"
      ],
      "id": "lS4f4yzOE5sk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### func_pred"
      ],
      "metadata": {
        "id": "m63Qgo2VdajP"
      },
      "id": "m63Qgo2VdajP"
    },
    {
      "cell_type": "code",
      "source": [
        "def pronumtoname(x, le):\n",
        "    return (\"\".join(le.inverse_transform([x])[0].split()))\n",
        "\n",
        "def make_songpool_for_eachpro(pro_id, train_final, test_final, train_df, test_df):\n",
        "    # 사람용 song pool\n",
        "    song_book_all= pd.concat([train_final, test_final], axis=0, ignore_index=True)\n",
        "    song_book = song_book_all[['song_id','songName','artistName','album','release','genre']]\n",
        "    song_book = song_book.drop_duplicates(subset=['song_id'])\n",
        "\n",
        "    # song pool\n",
        "    song_pool_all= pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
        "    song_pool = song_pool_all[['genre','release','song_id']]\n",
        "    song_pool = song_pool.drop_duplicates(subset=['song_id'])\n",
        "\n",
        "    # sampling weather conditions for a test\n",
        "    df_all = pd.concat([train_df, test_df], axis=0)\n",
        "    test_pred_d = df_all.sample(3)\n",
        "    test_pred_days = test_pred_d[['weather_clear', 'weather_cloud',\n",
        "          'weather_fog', 'weather_rain', 'weather_snow', 'weather_thunder',\n",
        "          'weather_disaster','avg_temp','rain','temp_diff','month','day']]\n",
        "    p_id = pro_id\n",
        "    ProgramName = pd.DataFrame({'ProgramName':[p_id, p_id, p_id]})\n",
        "    test_pred_days['ProgramName'] = [p_id, p_id, p_id]\n",
        "\n",
        "    colname_days = ['ProgramName','weather_clear', 'weather_cloud',\n",
        "          'weather_fog', 'weather_rain', 'weather_snow', 'weather_thunder',\n",
        "          'weather_disaster','avg_temp','rain','temp_diff','month','day']\n",
        "    test_pred_days = test_pred_days[colname_days]\n",
        "\n",
        "    # test_pred = test_pred_days.merge(song_pool, how='cross') #python version 1.2> needed\n",
        "    test_pred_days['key'] = 1\n",
        "    song_pool['key'] = 1\n",
        "    test_pred = pd.merge(test_pred_days, song_pool, on ='key').drop(\"key\", 1)\n",
        "\n",
        "    colname = ['ProgramName', 'weather_clear', 'weather_cloud',\n",
        "          'weather_fog', 'weather_rain', 'weather_snow', 'weather_thunder',\n",
        "          'weather_disaster', 'avg_temp', 'genre',\n",
        "           'release', 'song_id', 'rain', 'temp_diff', 'month', 'day']\n",
        "    test_pred = test_pred[colname]\n",
        "    return test_pred, song_book\n",
        "\n",
        "def make_pred_list(batch, pro_id, model, test_pred, song_book, scaler, le):\n",
        "    # test_pred_scaled = test_pred.astype('float')\n",
        "    test_pred_scaled = scaler.transform(test_pred)\n",
        "    batch = batch\n",
        "    test_pred_tensor = tf.data.Dataset.from_tensor_slices(\n",
        "            (tf.cast(test_pred_scaled, tf.float32))).batch(batch)\n",
        "\n",
        "    rslt_pred = []\n",
        "    for x in test_pred_tensor:\n",
        "    #     pred = model(x)\n",
        "        # rslt_pred.append(model(x))\n",
        "        rslt_pred.append(model(x)[0])\n",
        "    rslt = sum([i.numpy().tolist() for i in rslt_pred], [])\n",
        "    # rslt = sum(rslt_pred, [])\n",
        "\n",
        "    test_pred['pred'] = rslt\n",
        "\n",
        "    test_pred_dropped = test_pred.drop(['genre','release'], axis=1)\n",
        "    test_pred_merged = pd.merge(test_pred_dropped, song_book, left_on='song_id', right_on='song_id', how='left')\n",
        "\n",
        "    p_id = pro_id\n",
        "    test_pred_merged['Program']=pronumtoname(p_id, le)\n",
        "    # test_pred_merged['Program']=test_pred_merged['ProgramName'].apply(pronumtoname)\n",
        "\n",
        "    return test_pred_merged"
      ],
      "metadata": {
        "id": "tO_Hr7DlckCB"
      },
      "id": "tO_Hr7DlckCB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# day별로 나누기\n",
        "def day_by_day(test_pred_merged):\n",
        "  len1 = (len(test_pred_merged)/3 - 1)\n",
        "  len2 = (len(test_pred_merged)*2/3 - 1)\n",
        "\n",
        "  day1 = test_pred_merged.loc[:len1]\n",
        "  day2 = test_pred_merged.loc[(len1+1):len2]\n",
        "  day3 = test_pred_merged.loc[(len2+1):]\n",
        "  return day1, day2, day3"
      ],
      "metadata": {
        "id": "gio5Nn_Xtm2E"
      },
      "id": "gio5Nn_Xtm2E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 메인 함수"
      ],
      "metadata": {
        "id": "UYdZmtAHAFC6"
      },
      "id": "UYdZmtAHAFC6"
    },
    {
      "cell_type": "code",
      "source": [
        "def main_func(user_df, songDB, tbn, p_id, batch_size, w1, w2):\n",
        "  dat_, programs_id, le, train_df, test_df, col_accum_index_dict, train_final, test_final = preprocess(user_df, songDB, tbn) #전처리\n",
        "  fm_model = func_fm(train_df, test_df)\n",
        "  recent_song_similar_items, recommendations = combine_func1_func2(train_final, dat_) #func1, func2 실행 후 합침\n",
        "  # p_id = 256\n",
        "  # batch_size = 512\n",
        "\n",
        "  scaler = fm_model[1]\n",
        "  test_pred, song_book = make_songpool_for_eachpro(p_id, train_final, test_final, train_df, test_df)\n",
        "  results_pro_pid = make_pred_list(batch_size, p_id, fm_model[0][1], test_pred, song_book, scaler, le)\n",
        "\n",
        "  df_similar = pd.DataFrame({'song_id':[int(x) for x in recent_song_similar_items[p_id]], 'similar':1})\n",
        "  df_popular = pd.DataFrame({'song_id':[int(x) for x in recommendations[p_id]], 'popular':1})\n",
        "  # df_merged = pd.merge(day1, df_similar, how='left', on='song_id')\n",
        "  df_merged = pd.merge(results_pro_pid, df_similar, how='left', on='song_id')\n",
        "  df_merged = pd.merge(df_merged, df_popular, how='left', on='song_id')\n",
        "  df_merged['similar'] = df_merged['similar'].fillna(0)\n",
        "  df_merged['popular'] = df_merged['popular'].fillna(0)\n",
        "\n",
        "  # w1 = 0.4\n",
        "  # w2 = 0.2\n",
        "  df_merged['pred_fin'] = w1 * df_merged['similar'] + w2 * df_merged['popular'] + df_merged['pred']\n",
        "  return df_merged"
      ],
      "metadata": {
        "id": "b7wVfAYPunq0"
      },
      "id": "b7wVfAYPunq0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = main_func(user_df, songDB, tbn, 256, 512, 0.4, 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twdWxrj1VsQ6",
        "outputId": "61434d57-2d26-460a-87a8-abff72dd16e0"
      },
      "id": "twdWxrj1VsQ6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:541: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:542: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:548: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:549: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data transformaiton completed\n",
            "Restored from /content/drive/MyDrive/temp/fm_check/ckpt-62\n",
            "Saved checkpoint for step 64: /content/drive/MyDrive/temp/fm_check/ckpt-63\n",
            "스텝 000에서 RMSE: 0.9651\n",
            "Saved checkpoint for step 65: /content/drive/MyDrive/temp/fm_check/ckpt-64\n",
            "스텝 001에서 RMSE: 0.9650\n",
            "Saved checkpoint for step 66: /content/drive/MyDrive/temp/fm_check/ckpt-65\n",
            "스텝 002에서 RMSE: 0.9651\n",
            "Saved checkpoint for step 67: /content/drive/MyDrive/temp/fm_check/ckpt-66\n",
            "스텝 003에서 RMSE: 0.9651\n",
            "Saved checkpoint for step 68: /content/drive/MyDrive/temp/fm_check/ckpt-67\n",
            "스텝 004에서 RMSE: 0.9651\n",
            "테스트 RMSE: 0.9656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 완성된 결과 적용(추천 실행)"
      ],
      "metadata": {
        "id": "Qac93tpCAV6h"
      },
      "id": "Qac93tpCAV6h"
    },
    {
      "cell_type": "code",
      "source": [
        "day1, day2, day3 = day_by_day(df_final)"
      ],
      "metadata": {
        "id": "oA-YEbnatv5P"
      },
      "id": "oA-YEbnatv5P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "day1[['Program', 'songName', 'artistName', 'release', 'genre', 'pred_fin']].sort_values(by='pred_fin', ascending=False).head(30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "gLztVexlt0tN",
        "outputId": "02a30203-ff22-454f-d9a4-3b4338fa1441"
      },
      "id": "gLztVexlt0tN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9bac1ca3-be0d-42a8-9718-e7372756e8db\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Program</th>\n",
              "      <th>songName</th>\n",
              "      <th>artistName</th>\n",
              "      <th>release</th>\n",
              "      <th>genre</th>\n",
              "      <th>pred_fin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1730</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>세월이 가면</td>\n",
              "      <td>최호섭</td>\n",
              "      <td>1985</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>3.065925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1731</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>세월이 가면</td>\n",
              "      <td>최호섭</td>\n",
              "      <td>1985</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>3.065925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1160</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>하늘을 달리다</td>\n",
              "      <td>이적</td>\n",
              "      <td>2000</td>\n",
              "      <td>Rock</td>\n",
              "      <td>3.046499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1547</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>bravo my life</td>\n",
              "      <td>봄여름가을겨울</td>\n",
              "      <td>2000</td>\n",
              "      <td>Rock</td>\n",
              "      <td>3.045370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1548</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>bravo my life</td>\n",
              "      <td>봄여름가을겨울</td>\n",
              "      <td>2000</td>\n",
              "      <td>Rock</td>\n",
              "      <td>3.045370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>970</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>고마워요</td>\n",
              "      <td>임현정</td>\n",
              "      <td>2000</td>\n",
              "      <td>Rock</td>\n",
              "      <td>3.043594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1865</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>어디쯤 가고 있을까</td>\n",
              "      <td>전영</td>\n",
              "      <td>2000</td>\n",
              "      <td>Folk</td>\n",
              "      <td>3.027205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1864</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>어디쯤 가고 있을까</td>\n",
              "      <td>전영</td>\n",
              "      <td>2000</td>\n",
              "      <td>Folk</td>\n",
              "      <td>3.027205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5896</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>터</td>\n",
              "      <td>재주소년</td>\n",
              "      <td>2020</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>3.007779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12433</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>님</td>\n",
              "      <td>김정호</td>\n",
              "      <td>2000</td>\n",
              "      <td>adult</td>\n",
              "      <td>2.890665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2625</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>사랑하는 그대에게</td>\n",
              "      <td>유심초</td>\n",
              "      <td>1985</td>\n",
              "      <td>adult</td>\n",
              "      <td>2.886035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5909</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>텔레파시</td>\n",
              "      <td>도시아이들</td>\n",
              "      <td>1985</td>\n",
              "      <td>adult</td>\n",
              "      <td>2.885520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5455</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>골목길</td>\n",
              "      <td>이재민</td>\n",
              "      <td>1985</td>\n",
              "      <td>adult</td>\n",
              "      <td>2.885315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4456</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>그날</td>\n",
              "      <td>김연숙</td>\n",
              "      <td>1995</td>\n",
              "      <td>adult</td>\n",
              "      <td>2.877393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>낭만에 대하여</td>\n",
              "      <td>최백호</td>\n",
              "      <td>1995</td>\n",
              "      <td>adult</td>\n",
              "      <td>2.877375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5190</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>널 사랑하니까</td>\n",
              "      <td>신승훈</td>\n",
              "      <td>1990</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>2.877122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2211</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>난 널 사랑해</td>\n",
              "      <td>신효범</td>\n",
              "      <td>1990</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>2.877094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6758</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>나 어떡해</td>\n",
              "      <td>샌드 페블즈</td>\n",
              "      <td>1995</td>\n",
              "      <td>adult</td>\n",
              "      <td>2.877075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4589</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>하늘</td>\n",
              "      <td>박선희</td>\n",
              "      <td>1990</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>2.877011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6327</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>가을이 오면</td>\n",
              "      <td>이문세</td>\n",
              "      <td>1990</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>2.876667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3403</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>그리운 얼굴</td>\n",
              "      <td>유익종</td>\n",
              "      <td>1995</td>\n",
              "      <td>adult</td>\n",
              "      <td>2.876536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1889</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>사랑을 위하여</td>\n",
              "      <td>김종환</td>\n",
              "      <td>1995</td>\n",
              "      <td>adult</td>\n",
              "      <td>2.876209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3109</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>가려진 시간 사이로</td>\n",
              "      <td>윤상</td>\n",
              "      <td>1990</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>2.875771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7949</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>미니데이트</td>\n",
              "      <td>윤영아</td>\n",
              "      <td>1990</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>2.874207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2986</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>너를 위해</td>\n",
              "      <td>임재범</td>\n",
              "      <td>2000</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>2.873519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1007</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>사랑해도 될까요</td>\n",
              "      <td>유리상자</td>\n",
              "      <td>2000</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>2.873421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5498</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>사랑은 연필로 쓰세요</td>\n",
              "      <td>전영록</td>\n",
              "      <td>1985</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>2.869823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2906</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>종이학</td>\n",
              "      <td>전영록</td>\n",
              "      <td>1985</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>2.869823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2907</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>종이학</td>\n",
              "      <td>전영록</td>\n",
              "      <td>1985</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>2.869823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>한밤의교차로</td>\n",
              "      <td>기차와 소나무</td>\n",
              "      <td>이규석</td>\n",
              "      <td>1985</td>\n",
              "      <td>Ballade</td>\n",
              "      <td>2.869818</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9bac1ca3-be0d-42a8-9718-e7372756e8db')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9bac1ca3-be0d-42a8-9718-e7372756e8db button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9bac1ca3-be0d-42a8-9718-e7372756e8db');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Program       songName artistName  release    genre  pred_fin\n",
              "1730   한밤의교차로         세월이 가면        최호섭     1985  Ballade  3.065925\n",
              "1731   한밤의교차로         세월이 가면        최호섭     1985  Ballade  3.065925\n",
              "1160   한밤의교차로        하늘을 달리다         이적     2000     Rock  3.046499\n",
              "1547   한밤의교차로  bravo my life    봄여름가을겨울     2000     Rock  3.045370\n",
              "1548   한밤의교차로  bravo my life    봄여름가을겨울     2000     Rock  3.045370\n",
              "970    한밤의교차로           고마워요        임현정     2000     Rock  3.043594\n",
              "1865   한밤의교차로     어디쯤 가고 있을까         전영     2000     Folk  3.027205\n",
              "1864   한밤의교차로     어디쯤 가고 있을까         전영     2000     Folk  3.027205\n",
              "5896   한밤의교차로              터       재주소년     2020  Ballade  3.007779\n",
              "12433  한밤의교차로              님        김정호     2000    adult  2.890665\n",
              "2625   한밤의교차로      사랑하는 그대에게        유심초     1985    adult  2.886035\n",
              "5909   한밤의교차로           텔레파시      도시아이들     1985    adult  2.885520\n",
              "5455   한밤의교차로            골목길        이재민     1985    adult  2.885315\n",
              "4456   한밤의교차로             그날        김연숙     1995    adult  2.877393\n",
              "1191   한밤의교차로        낭만에 대하여        최백호     1995    adult  2.877375\n",
              "5190   한밤의교차로        널 사랑하니까        신승훈     1990  Ballade  2.877122\n",
              "2211   한밤의교차로        난 널 사랑해        신효범     1990  Ballade  2.877094\n",
              "6758   한밤의교차로          나 어떡해     샌드 페블즈     1995    adult  2.877075\n",
              "4589   한밤의교차로             하늘        박선희     1990  Ballade  2.877011\n",
              "6327   한밤의교차로         가을이 오면        이문세     1990  Ballade  2.876667\n",
              "3403   한밤의교차로         그리운 얼굴        유익종     1995    adult  2.876536\n",
              "1889   한밤의교차로        사랑을 위하여        김종환     1995    adult  2.876209\n",
              "3109   한밤의교차로     가려진 시간 사이로         윤상     1990  Ballade  2.875771\n",
              "7949   한밤의교차로          미니데이트        윤영아     1990  Ballade  2.874207\n",
              "2986   한밤의교차로          너를 위해        임재범     2000  Ballade  2.873519\n",
              "1007   한밤의교차로       사랑해도 될까요       유리상자     2000  Ballade  2.873421\n",
              "5498   한밤의교차로    사랑은 연필로 쓰세요        전영록     1985  Ballade  2.869823\n",
              "2906   한밤의교차로            종이학        전영록     1985  Ballade  2.869823\n",
              "2907   한밤의교차로            종이학        전영록     1985  Ballade  2.869823\n",
              "285    한밤의교차로        기차와 소나무        이규석     1985  Ballade  2.869818"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Sru-zKewEuJy",
        "PR2GZWRvVhZG",
        "M6SQaNeLwDTN",
        "Hrus4qrbydz-",
        "Ktt3jzC5zBYg",
        "m63Qgo2VdajP"
      ],
      "machine_shape": "hm",
      "name": "추천시스템_tbn_rating_fin_0124",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}